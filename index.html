<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="MultiSum: A Dataset for Multimodal Video Summarization and Thumbnail Generation">
    <meta name="author"
          content="Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Bo Li, Ding Zhao, Lijuan Wang">

    <title>MultiSum: A Dataset for Multimodal Video Summarization and Thumbnail Generation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>MultiSum: A Dataset for Multimodal Video Summarization and Thumbnail Generation</h2>
    <hr>
    <p class="authors">
        <a href="https://www.cs.cmu.edu/~jielinq/">Jielin Qiu<sup>1,2</sup></a>,
        <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu<sup>1</sup></a>,
        <a href="https://willxxy.github.io/">William Han<sup>1</sup></a>,
        <a href="">Aditesh Kumar<sup>1</sup></a>,
        <a href="">Karthik Mittal<sup>1</sup></a>,
        <a href="">Claire Jin<sup>1</sup></a>,
    </p>
    <p class="authors">
        <a href="https://zyang-ur.github.io/">Zhengyuan Yang<sup>2</sup></a>,
        <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li<sup>2</sup></a>,
        <a href="https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en">Jianfeng Wang<sup>2</sup></a>,
        <a href="https://aisecure.github.io/">Bo Li<sup>3</sup></a>,
        <a href="https://safeai-lab.github.io/">Ding Zhao<sup>1</sup></a>,
        <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=en">Lijuan Wang<sup>2</sup></a>
    </p>
    <p class="authors">
        <sup>1</sup>Carnegie Mellon University,
        <sup>2</sup>Microsoft Azure AI,
        <sup>3</sup>University of Illinois Urbana-Champaign
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="">Data[coming soon]</a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="">Paper[coming soon]</a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/Jason-Qiu/MultiSum">Data Collection Tool</a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="">Model Code</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="images/multisum_categories.png" style="width:80%">
            </div>
        </div>
        <p>
         The 17 main categories of the MultiSum dataset, where each main category contains 10 subcategories, resulting in 170 subcategories in total.
        </p>
    </div>

    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
          The realm of multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction with significant potential for real-world applications. For instance, it can facilitate the automated generation of cover images and titles for multimedia content or provide introductions to online videos. However, existing datasets encounter several issues, such as inadequate maintenance, unavailability of data, small size, lack of categorization, and insufficiency for effective learning. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated a novel dataset called MultiSum. It includes both human-verified video summaries and textual summaries.
An important aspect of the MultiSum dataset is its extensive categorization. Videos within the dataset have been collected across 17 main categories, each containing 10 subcategories, resulting in a total of 170 subcategories. This categorization ensures that the MultiSum dataset is not only representative but also well-structured.
Furthermore, we have conducted benchmarking experiments using the proposed dataset, evaluating various tasks and methods such as video temporal segmentation, video summarization, text summarization, and multimodal summarization. 
In our commitment to fostering accessibility and collaboration, we have made the MultiSum dataset and the data collection tool fully open-source. This initiative allows researchers and practitioners to freely access and utilize the dataset, promoting transparency and facilitating further advancements in the field.
        </p>
    </div>

    <br>


    <div class="section">
        <h2>MultiSum Data Statistics</h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="images/multisum_statistics.png" style="width:100%">
            </div>
        </div>
        <p>
            The statistics of the MultiSum dataset, which show the distribution of (a) video duration; (b) number of segments per video; (c) segment duration; (d) number of words per sentence.
        </p>
    </div>

    <br>



    <div class="section">
        <h2>Task Comparison</h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="images/task_compare.png" style="width:100%">
            </div>
        </div>
        <p>
            Task comparison of traditional video summarization, text summarization, and the new MSMO (multimodal summarization with multimodal output) task.
        </p>

        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="images/dataset_comparison.png" style="width:100%">
            </div>
        </div>
        <p>
            Comparison with existing video summarization and multimodal summarization datasets.
        </p>
    </div>

    <br>



    <div class="section">
        <h2>MultiSum Benchmark Pipeline</h2>

        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="images/pipeline.png" style="width:100%">
            </div>
        </div>
        <p>
            The design of the proposed MultiSum dataset is driven by research and application needs.
        </p>
    </div>

    <br>

    <div class="section">
        <h2>Comparison of the modality of different summarization tasks and datasets</h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="images/comparison_table.png" style="width:100%">
            </div>
        </div>
        <p>
            Comparison of the modality of different summarization tasks and datasets, where MSMO is short for Multimodal Summarization with Multimodal Output. The major difference between traditional multimodal summarization and MSMO is that traditional multimodal summarization still outputs a single-modality summary, while MSMO outputs both modalities' summaries.
        </p>

    </div>

    <br>


    <br>


    <hr>

    <footer>
        <p>
            Acknowledgement: This page is modified from <a href="https://yilundu.github.io/">Yilun Du</a>.
        </p>
    </footer>




    

</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>